{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PySpark**: The Apache Spark Python API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "This notebook shows how to connect Jupyter notebooks to a Spark cluster to process data using Spark Python API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Spark Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Connection\n",
    "\n",
    "To connect to the Spark cluster, create a SparkSession object with the following params:\n",
    "\n",
    "+ **appName:** application name displayed at the [Spark Master Web UI](http://localhost:8080/);\n",
    "+ **master:** Spark Master URL, same used by Spark Workers;\n",
    "+ **spark.executor.memory:** must be less than or equals to docker compose SPARK_WORKER_MEMORY config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/08 14:25:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"pyspark-notebook\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"512m\").\\\n",
    "        getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More confs for SparkSession object in standalone mode can be added using the **config** method. Checkout the API docs [here](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SparkSession)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites = sc.textFile(\"/data/websites.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['website',\n",
       " 'vimeo.com',\n",
       " 'springer.com',\n",
       " 'youtube.com',\n",
       " 'marriott.com',\n",
       " 'nytimes.com',\n",
       " 'microsoft.com',\n",
       " 'washingtonpost.com',\n",
       " 'bloomberg.com',\n",
       " 'bbc.com',\n",
       " 'thestartmagazine.com']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "websites.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to capture HTML of a website and return as tuple\n",
    "def capture_html(url):\n",
    "    import requests\n",
    "    response = requests.get(r'https://www.' + url)\n",
    "    return (url, response.text)\n",
    "\n",
    "# Filter out the first element of the RDD\n",
    "websites_filtered = websites.filter(lambda x: x != 'website')\n",
    "\n",
    "# Apply the function to each URL in the filtered RDD using the map function\n",
    "website_data = websites_filtered.map(capture_html)\n",
    "\n",
    "# Convert the RDD to a DataFrame and save as a CSV file\n",
    "df = website_data.toDF([\"url\", \"html\"])\n",
    "df.write.format(\"csv\").save(\"/data/file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_website(website):\n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "    from webdriver_manager.chrome import ChromeDriverManager\n",
    "    import time\n",
    "\n",
    "    from pyvirtualdisplay import Display\n",
    "    import pandas as pd\n",
    "    import requests\n",
    "    import os\n",
    "    import sys\n",
    "\n",
    "\n",
    "    # virtual display\n",
    "    display = Display(visible=0, size=(800, 600))\n",
    "    display.start()\n",
    "\n",
    "    # extension filepath\n",
    "    ext_file = \"/usr/bin/spark-3.0.0-bin-hadoop3.2/data/extension\"\n",
    "\n",
    "    opt = webdriver.ChromeOptions()\n",
    "    # devtools necessary for complete network stack capture\n",
    "    opt.add_argument(\"--auto-open-devtools-for-tabs\")\n",
    "    # loads extension\n",
    "    opt.add_argument(\"load-extension=\" + ext_file)\n",
    "    # important for linux\n",
    "    opt.add_argument(\"--no-sandbox\")\n",
    "    opt.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "    dc = DesiredCapabilities.CHROME\n",
    "    dc[\"goog:loggingPrefs\"] = {\"browser\": \"ALL\"}\n",
    "\n",
    "    os.mkdir(\"/usr/bin/spark-3.0.0-bin-hadoop3.2/data/server/output/\" + website)\n",
    "    driver = webdriver.Chrome(\n",
    "        ChromeDriverManager().install(), options=opt, desired_capabilities=dc\n",
    "    )\n",
    "    requests.post(\n",
    "        url=\"http://localhost:3000/complete\", data={\"website\": website}\n",
    "    )\n",
    "    driver.get(r\"https://www.\" + website)\n",
    "    time.sleep(5)\n",
    "\n",
    "    # driver.quit\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the first element of the RDD\n",
    "websites_filtered = websites.filter(lambda x: x != 'website')\n",
    "\n",
    "# Apply the function to each URL in the filtered RDD using the map function\n",
    "website_data = websites_filtered.map(crawl_website)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "website_data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
